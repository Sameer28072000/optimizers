{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d08a6f-d0d2-4572-8df5-cfe148367476",
   "metadata": {},
   "source": [
    "Q1. What is the role of optimization algorithms in artificial neural networksK Why are they necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834fc51-fca6-47a1-9600-39cedcd16f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Optimization algorithms play a crucial role in training artificial neural networks.\n",
    "       Role of optimization algorithms in artificial neural networks:\n",
    "          i.Parameter update\n",
    "            ii.Minimization of loss function\n",
    "              iii.Dealing with large scale optimization\n",
    "                iv.Handling incomplete data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a888c-8a8e-4deb-af8f-97775584912b",
   "metadata": {},
   "source": [
    "Q2.Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms\n",
    "of convergence speed and memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbee56-676d-4812-a905-3ccab77725f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Gradient descent is a widely used optimization algorithm in machine learning, including artificial neural networks.\n",
    "       There are different variants of gradient descent:\n",
    "         i.Batch Gradient Descent (BGD)\n",
    "            ii.Stochastic Gradient Descent (SGD)\n",
    "               iii.Mini-Batch Gradient Descent\n",
    "                  iv.AdaGrad\n",
    "                    \n",
    "            Convergence Speed:\n",
    "                 BGD typically converges slowly but can guarantee convergence to the global minimum for convex problems.\n",
    "                    SGD and mini-batch gradient descent converge faster.\n",
    "                    \n",
    "            Memory Requirements: \n",
    "                BGD requires memory to store the entire dataset for gradient computation, making it memory-intensive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd35fd-54a1-4b1a-ba78-ce7772e24a82",
   "metadata": {},
   "source": [
    "Q3.Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow\n",
    "convergence, local minima<. How do modern optimizers address these challenges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc980f1-2e64-44a3-871e-4baa48774eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:\n",
    "    Traditional gradient descent optimization methods, such as batch gradient descent (BGD), \n",
    "      stochastic gradient descent (SGD), and mini-batch gradient descent.\n",
    "        \n",
    "        Some challenges and how modern optimizers tackle them:\n",
    "            i.slow convergence\n",
    "              ii.local minima\n",
    "                iii.learning rate selection\n",
    "                \n",
    "    modern optimizers are designed to address the challenges associated with traditional gradient descent methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1946a9c-813b-49cc-b80e-f331b2a7ebcf",
   "metadata": {},
   "source": [
    "Q4.Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do\n",
    "they impact convergence and model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296a0a3-1356-40b2-a529-cb0566212c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Momentum:\n",
    "        Momentum is a technique used in optimization algorithms to improve convergence speed and stability.\n",
    "         In each iteration, the gradient is computed for the current set of parameters.\n",
    "    Benefits of using momentum:\n",
    "        i.Faster convergence\n",
    "          ii.Smoother parameters updates\n",
    "            \n",
    "        Learning Rate:\n",
    "            The learning rate is a hyperparameter that determines the step size taken during each \n",
    "                parameter update in an optimization algorithm.\n",
    "                \n",
    "        Impact on convergence and model performance:\n",
    "            i.convergence speed\n",
    "              ii.stability\n",
    "                iii.generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9731bac-d304-4a74-941e-824651f4a965",
   "metadata": {},
   "source": [
    "Q5.Explain the concept of Stochastic radient Descent (SGD) and its advantages compared to traditional\n",
    "gradient descent. Discuss its limitations and scenarios where it is most suitablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e53da4-9556-4bb0-980a-9dda4cc15529",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Stochastic Gradient Descent (SGD) is an optimization algorithm commonly used in machine learning,\n",
    "        particularly in large-scale and online learning scenarios.\n",
    "    \n",
    "    Advantages of SGD over traditional gradient descent:\n",
    "        i.convergence speed\n",
    "          ii.exploration of solution space\n",
    "            \n",
    "            \n",
    "    Limitations and scenarios where SGD is most suitable:\n",
    "        i.Noisy gradient estimates\n",
    "          ii.increase variance\n",
    "            iii.suitable for large data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e800f-d8c9-4e44-b0fe-d91317a47e6c",
   "metadata": {},
   "source": [
    "Q6.Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates.\n",
    "Discuss its benefits and potential drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb1eda-c439-4328-ae2d-6b793010d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans: Adam(Adaptive Moment Estimation) optimizer is a popular optimization algorithm that combines \n",
    "       the concepts of momentum and adaptive learning rates.\n",
    "    Benefits of Adam optimizer:\n",
    "        i.Adaptive learning rates\n",
    "          ii.Momentum\n",
    "            iii.robustness to different hyperparameter\n",
    "            \n",
    "    Potential drawbacks of Adam optimizer:\n",
    "        i.Increased memory requirement\n",
    "          ii.Hyperparameter sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034af2e3-d427-4ec6-b707-58cac5a1ee53",
   "metadata": {},
   "source": [
    "Q7.Explain the concept of RMSprop optimizer and how it addresses the challenges of adaptive learning\n",
    "rates.compare it with Adam and discuss their relative strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf5304-cbf7-48cd-ad66-3dbc252bc106",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-\n",
    "      RMSprop (Root Mean Square Propagation) is an optimization algorithm that addresses \n",
    "         the challenges of adaptive learning rates.\n",
    "         \n",
    "        Comparison of RMSprop and Adam optimizers:\n",
    "            i.Adaptive learning rates\n",
    "              ii.Momentum\n",
    "                iii.Strength \n",
    "                   iv.Weakness\n",
    "                    \n",
    "            The choice between RMSprop and Adam depends on the specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bb4d5-9df1-4cb7-8e23-6e4f5b282dd9",
   "metadata": {},
   "source": [
    "Q8.Implement SD, Adam, and RMSprop optimizers in a deep learning model using a framework of your\n",
    "choice. Train the model on a suitable dataset and compare their impact on model convergence and\n",
    "performancen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7621e9cf-0a42-44bc-baf5-df754d3f978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 242728267.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 53340243.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 185025720.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 9754495.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Training with SGD optimizer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 2.2805\n",
      "Epoch [1/10], Step [200/938], Loss: 2.2477\n",
      "Epoch [1/10], Step [300/938], Loss: 2.2125\n",
      "Epoch [1/10], Step [400/938], Loss: 2.1413\n",
      "Epoch [1/10], Step [500/938], Loss: 2.1515\n",
      "Epoch [1/10], Step [600/938], Loss: 2.1004\n",
      "Epoch [1/10], Step [700/938], Loss: 2.0610\n",
      "Epoch [1/10], Step [800/938], Loss: 1.9551\n",
      "Epoch [1/10], Step [900/938], Loss: 1.9779\n",
      "Epoch [2/10], Step [100/938], Loss: 1.8091\n",
      "Epoch [2/10], Step [200/938], Loss: 1.8398\n",
      "Epoch [2/10], Step [300/938], Loss: 1.6574\n",
      "Epoch [2/10], Step [400/938], Loss: 1.6497\n",
      "Epoch [2/10], Step [500/938], Loss: 1.4749\n",
      "Epoch [2/10], Step [600/938], Loss: 1.4132\n",
      "Epoch [2/10], Step [700/938], Loss: 1.3056\n",
      "Epoch [2/10], Step [800/938], Loss: 1.1977\n",
      "Epoch [2/10], Step [900/938], Loss: 1.1796\n",
      "Epoch [3/10], Step [100/938], Loss: 1.0507\n",
      "Epoch [3/10], Step [200/938], Loss: 1.0147\n",
      "Epoch [3/10], Step [300/938], Loss: 0.9252\n",
      "Epoch [3/10], Step [400/938], Loss: 0.9694\n",
      "Epoch [3/10], Step [500/938], Loss: 0.7297\n",
      "Epoch [3/10], Step [600/938], Loss: 1.0042\n",
      "Epoch [3/10], Step [700/938], Loss: 0.8935\n",
      "Epoch [3/10], Step [800/938], Loss: 0.6166\n",
      "Epoch [3/10], Step [900/938], Loss: 0.7488\n",
      "Epoch [4/10], Step [100/938], Loss: 0.8367\n",
      "Epoch [4/10], Step [200/938], Loss: 0.5958\n",
      "Epoch [4/10], Step [300/938], Loss: 0.4940\n",
      "Epoch [4/10], Step [400/938], Loss: 0.5687\n",
      "Epoch [4/10], Step [500/938], Loss: 0.5609\n",
      "Epoch [4/10], Step [600/938], Loss: 0.6467\n",
      "Epoch [4/10], Step [700/938], Loss: 0.7582\n",
      "Epoch [4/10], Step [800/938], Loss: 0.4841\n",
      "Epoch [4/10], Step [900/938], Loss: 0.4752\n",
      "Epoch [5/10], Step [100/938], Loss: 0.5871\n",
      "Epoch [5/10], Step [200/938], Loss: 0.4369\n",
      "Epoch [5/10], Step [300/938], Loss: 0.5026\n",
      "Epoch [5/10], Step [400/938], Loss: 0.4284\n",
      "Epoch [5/10], Step [500/938], Loss: 0.3742\n",
      "Epoch [5/10], Step [600/938], Loss: 0.5315\n",
      "Epoch [5/10], Step [700/938], Loss: 0.4097\n",
      "Epoch [5/10], Step [800/938], Loss: 0.5070\n",
      "Epoch [5/10], Step [900/938], Loss: 0.4606\n",
      "Epoch [6/10], Step [100/938], Loss: 0.4200\n",
      "Epoch [6/10], Step [200/938], Loss: 0.4247\n",
      "Epoch [6/10], Step [300/938], Loss: 0.3728\n",
      "Epoch [6/10], Step [400/938], Loss: 0.4806\n",
      "Epoch [6/10], Step [500/938], Loss: 0.3648\n",
      "Epoch [6/10], Step [600/938], Loss: 0.4161\n",
      "Epoch [6/10], Step [700/938], Loss: 0.3028\n",
      "Epoch [6/10], Step [800/938], Loss: 0.3589\n",
      "Epoch [6/10], Step [900/938], Loss: 0.4967\n",
      "Epoch [7/10], Step [100/938], Loss: 0.3364\n",
      "Epoch [7/10], Step [200/938], Loss: 0.4221\n",
      "Epoch [7/10], Step [300/938], Loss: 0.4379\n",
      "Epoch [7/10], Step [400/938], Loss: 0.3343\n",
      "Epoch [7/10], Step [500/938], Loss: 0.3883\n",
      "Epoch [7/10], Step [600/938], Loss: 0.4791\n",
      "Epoch [7/10], Step [700/938], Loss: 0.5124\n",
      "Epoch [7/10], Step [800/938], Loss: 0.5976\n",
      "Epoch [7/10], Step [900/938], Loss: 0.4660\n",
      "Epoch [8/10], Step [100/938], Loss: 0.3518\n",
      "Epoch [8/10], Step [200/938], Loss: 0.5632\n",
      "Epoch [8/10], Step [300/938], Loss: 0.4364\n",
      "Epoch [8/10], Step [400/938], Loss: 0.4161\n",
      "Epoch [8/10], Step [500/938], Loss: 0.2920\n",
      "Epoch [8/10], Step [600/938], Loss: 0.3931\n",
      "Epoch [8/10], Step [700/938], Loss: 0.2858\n",
      "Epoch [8/10], Step [800/938], Loss: 0.4734\n",
      "Epoch [8/10], Step [900/938], Loss: 0.2736\n",
      "Epoch [9/10], Step [100/938], Loss: 0.4245\n",
      "Epoch [9/10], Step [200/938], Loss: 0.4429\n",
      "Epoch [9/10], Step [300/938], Loss: 0.2027\n",
      "Epoch [9/10], Step [400/938], Loss: 0.3052\n",
      "Epoch [9/10], Step [500/938], Loss: 0.3472\n",
      "Epoch [9/10], Step [600/938], Loss: 0.2238\n",
      "Epoch [9/10], Step [700/938], Loss: 0.2770\n",
      "Epoch [9/10], Step [800/938], Loss: 0.2849\n",
      "Epoch [9/10], Step [900/938], Loss: 0.3282\n",
      "Epoch [10/10], Step [100/938], Loss: 0.5397\n",
      "Epoch [10/10], Step [200/938], Loss: 0.4736\n",
      "Epoch [10/10], Step [300/938], Loss: 0.2446\n",
      "Epoch [10/10], Step [400/938], Loss: 0.3348\n",
      "Epoch [10/10], Step [500/938], Loss: 0.3688\n",
      "Epoch [10/10], Step [600/938], Loss: 0.3740\n",
      "Epoch [10/10], Step [700/938], Loss: 0.2786\n",
      "Epoch [10/10], Step [800/938], Loss: 0.3860\n",
      "Epoch [10/10], Step [900/938], Loss: 0.2614\n",
      "Test Accuracy: 90.99%\n",
      "Training with Adam optimizer:\n",
      "Epoch [1/10], Step [100/938], Loss: 0.3454\n",
      "Epoch [1/10], Step [200/938], Loss: 0.1634\n",
      "Epoch [1/10], Step [300/938], Loss: 0.0758\n",
      "Epoch [1/10], Step [400/938], Loss: 0.2367\n",
      "Epoch [1/10], Step [500/938], Loss: 0.0670\n",
      "Epoch [1/10], Step [600/938], Loss: 0.0392\n",
      "Epoch [1/10], Step [700/938], Loss: 0.0453\n",
      "Epoch [1/10], Step [800/938], Loss: 0.1869\n",
      "Epoch [1/10], Step [900/938], Loss: 0.1409\n",
      "Epoch [2/10], Step [100/938], Loss: 0.0907\n",
      "Epoch [2/10], Step [200/938], Loss: 0.1022\n",
      "Epoch [2/10], Step [300/938], Loss: 0.1349\n",
      "Epoch [2/10], Step [400/938], Loss: 0.0825\n",
      "Epoch [2/10], Step [500/938], Loss: 0.0460\n",
      "Epoch [2/10], Step [600/938], Loss: 0.0542\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0497\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0979\n",
      "Epoch [2/10], Step [900/938], Loss: 0.1099\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0025\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0033\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0136\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0056\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0758\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0036\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0337\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0039\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0273\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0507\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0143\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0097\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0369\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0075\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0078\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0092\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0096\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0493\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0083\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0382\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0032\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0084\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0080\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0237\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0192\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0283\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0145\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0085\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0160\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0521\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0207\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0038\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0031\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0078\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0114\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0389\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0031\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0374\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0060\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0386\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0043\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0072\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0023\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0052\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0232\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0111\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0414\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0055\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0098\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0277\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0005\n",
      "Test Accuracy: 98.45%\n",
      "Training with RMS\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(14*14*32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sgd_optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "adam_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "rmsprop_optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch_idx+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print('Training with SGD optimizer:')\n",
    "train(sgd_optimizer)\n",
    "test()\n",
    "\n",
    "print('Training with Adam optimizer:')\n",
    "train(adam_optimizer)\n",
    "test()\n",
    "\n",
    "print('Training with RMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a194f06-5edf-4968-aa12-0baf2fd4f154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: torch==2.0.1 in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.101)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.8.8)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (4.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.12.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.4.0.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.91)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (65.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (0.38.4)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.6)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.26.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision) (1.2.1)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fe503-c2ad-4afc-a5d1-7e64781803b7",
   "metadata": {},
   "source": [
    "Q9.Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural\n",
    "network architecture and task. onsider factors such as convergence speed, stability, and\n",
    "generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9e191-a3dc-4f8c-a2d9-75c464f7e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-\n",
    "i.Convergence Speed: Different optimizers may have varying convergence speeds. \n",
    "                      some optimizers, like SGD, to more advanced optimizers like Adam or RMSprop.\n",
    "     \n",
    "ii.Stability: The stability of an optimizer refers to its ability to consistently find good solutions and \n",
    "                     avoid oscillations or divergences during training. \n",
    "    \n",
    "iii.Generalization Performance: Generalization refers to the ability of a trained model to perform well on unseen data. \n",
    "                                    The choice of optimizer can influence generalization performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
